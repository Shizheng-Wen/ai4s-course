- date: 9/18
  title: >
    Week 1 <strong>Course Introduction</strong> <a href="AISE25Lect1.pdf">[slides]</a> <a href="https://ethz.zoom.us/rec/play/f0vxnPqBJjP8fKcrx7qEnbD1ENg9kLxy6_D19hZFTz1qPIu_u6mmFDNujQyvIgItI5d8qzR6EhBivVEp.uj3NhRBr2hA-YGqe?eagerLoadZvaPages=sidemenu.billing.plan_management&accessLevel=meeting&canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fethz.zoom.us%2Frec%2Fshare%2FGand-InVqT_l0A2NDgQArhoI2o1nynVu7zuXCnlK2DNwb4ldTkO1tGV-tpfoag5u.sGt2HQ-r_eKp1aCE">[recording]</a>

  slides:
  topics:
    - Course syllabus and requirements
    - AI in Science and Engineering
    - Mathematical modeling with PDEs
    - Computational challenges
    - Motivation for AI approaches
  readings:
    - <a href="https://www.nature.com/articles/s41586-023-06221-2">Scientific discovery in the age of artificial intelligence</a> <br/>
    - <a href="https://www.nature.com/articles/s42254-024-00712-5">Neural operators for accelerating scientific simulations and design</a> <br/>
    - <a href="https://www.nature.com/articles/s42254-021-00314-5">Physics-informed machine learning</a> <br/>

- date: 9/25
  title: >
    Week 2 <strong>Introduction to Deep Learning</strong> <a href="AISE25Lect2.pdf">[slides]</a> <a href="https://ethz.zoom.us/rec/play/8QIKsLmZDtRTRjwauoXcSsg1ANkAc2D0jc8Munhj2BXFjAyuD3WhKnRuIolqPCSFh7-Pjny3DtIYuBI.5S9x3jgNVCXAJpcw?autoplay=true&startTime=1758780652000">[recording]</a>
  slides:
  topics:
    - Introduction to using deep learning to model physical systems governed by PDEs.
    - Structure of MLPs with layers, weights, biases, and activation functions (sigmoid, tanh, ReLU, etc.).
    - Gradient descent, stochastic gradient descent (SGD), mini-batch SGD
    - Motivation for convolutional neural networks (CNNs) to handle high-dimensional inputs efficiently.


  readings:
    - <a href="https://arxiv.org/abs/1412.6980">Adam:A Method for Stochastic Optimization</a> <br/>
    - <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"> WIKIPEDIA:Universal approximation theorem</a> <br/>
    - <a href="https://www.nature.com/articles/nature14539"> Deep learning</a> <br/>


- date: 10/2
  title: >
    Week 3 <strong>Introduction to Physics-Informed Neural Networks</strong> <a href="AISE25Lect3.pdf">[slides]</a> <a href="https://ethz.zoom.us/rec/play/fN2KHdgdgoIVkc-3JZfDf2BHKxQj58hwAqzkRaoc3aOr0ydN32-CK10sqKfsqJ0LzmbgsEUwi0G4nz4.SbdPCEmJbWFy_9eS?eagerLoadZvaPages=sidemenu.billing.plan_management&accessLevel=meeting&canPlayFromShare=true&from=share_recording_detail&startTime=1759385427000&componentName=rec-play&originRequestUrl=https%3A%2F%2Fethz.zoom.us%2Frec%2Fshare%2F_yp9-FA_eqb6QU5CQbcLoWy_tuLUyb_3H63yyDNpmLMazLcj8BPe7dYAmqs7FueY.1UVNnzP-eTSAP56_%3FstartTime%3D1759385427000">[recording]</a>
  slides:
  topics:
    - Introduction to Physics-Informed Neural Networks (PINNs).
    - Extending PINNs to reconstruct unknown solutions or parameters from partial measurements.
    - PINNs unify data-driven learning and physics-based modeling, offering flexible, mesh-free solvers for forward and inverse PDE problems.
  readings:
    - <a href="https://www.sciencedirect.com/science/article/pii/S0021999118307125">Physics-informed neural networks A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</a> <br/>
    - <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.1640100303">neural-network-based approximations for solving partial differential equations</a> <br/>
    - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=712178">Artificial Neural Networks for Solving Ordinary and Partial Differential Equations</a> <br/>
    - <a href="https://www.sciencedirect.com/science/article/pii/S0022407321001989">Physics informed neural networks for simulating radiative transfer</a> <br/>


- date: 10/9
  title: >
    Week 4 <strong>PINNs - Theoretical insights</strong> <a href="AISE25Lect4.pdf">[slides]</a> <a href="https://ethz.zoom.us/rec/play/kAAePCqakjFVOBbB8YPXMNRlQyUDyWudCl76fLNAa4j21qb9hj2ic2rvMYeqrz_YB4mYu4-GXv7ynLWj.4Ns8mOAcWNosJjKb?autoplay=true&startTime=1759990281000">[recording]</a>
  slides:
  topics:
    - Review of Physics-Informed Neural Networks (PINNs) for solving PDEs.
    - Theoretical analysis of PINN error - relation between training error, PDE residuals, and total approximation error.
    - Conditions ensuring convergence - coercivity, quadrature approximation, and DNN expressivity.
    - Rigorous error bounds for linear and nonlinear PDEs (Kolmogorov, Black-Scholes, Navier-Stokes, Burgers' equation).
    - Gradient descent dynamics and conditioning in PINN training; interpretation via NTK and preconditioning.
    - Practical performance and challenges:successes on smooth PDEs, difficulties on shocks or high-conditioning problems.
    - Overview of acceleration and stabilization techniques (causal learning, hard BCs, multi-stage networks).
  readings:
    - <a href="https://arxiv.org/abs/2106.14473">Error estimates for physics-informed neural networks approximating Kolmogorov PDEs</a> <br/>
    - <a href="https://arxiv.org/abs/2402.10926">Numerical analysis of physics-informed neural networks and related models in physics-informed machine learning</a> <br/>
    - <a href="https://arxiv.org/abs/2104.08938">On the approximation of functions by tanh neural networks</a> <br/>

- date: 10/16
  title: >
    Week 5 <strong>Operator Learning - Introduction</strong> <a href="AISE25Lect5.pdf">[slides]</a> <a href="https://ethz.zoom.us/rec/play/PA4-nX5frP8Sx5s1C0wlD784lHqHFNSxzGbzVyTYrg826Hj9ZOWtvsUhbAQvIEwMnda_5ei6Pb8nY35V.8Vk71uTJKFvXtq_5?autoplay=true&startTime=1760595311000">[recording]</a>
  slides:
  topics:
    - Transition from physics-informed learning to data-driven approaches.
    - Introduction to operator learning
    - Parametric PDE learning:deep networks approximating observables for low-dimensional parameterizations.
    - Operator learning:approximating infinite-dimensional mappings from data distributions.
    - Neural Operators generalizing DNNs to function spaces.
    - Fourier Neural Operators (FNOs):convolution in Fourier space, efficient and translation-invariant.
    - Theoretical foundation:universal approximation theorems for FNOs.
    - Practical challenges:bridging continuous operators and discrete numerical data (continuous-discrete equivalence).
  readings:
    - <a href="https://arxiv.org/abs/2108.08481">Neural Operator:Learning Maps Between Function Spaces</a> <br/>
    - <a href="https://arxiv.org/abs/2010.08895">Fourier Neural Operator for Parametric Partial Differential Equations</a> <br/>


- date: 10/23

- date: 10/30

- date: 11/6

- date: 11/13

- date: 11/20

- date: 11/27

- date: 12/4

- date: 12/11

- date: 12/18